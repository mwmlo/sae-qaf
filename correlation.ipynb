{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens.utils as utils\n",
    "from transformer_lens import ActivationCache, HookedTransformer\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import tools\n",
    "import latent_features\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", fold_ln=True)\n",
    "\n",
    "layer_index = 6\n",
    "location = \"mlp_post_act\"\n",
    "transformer_lens_loc = f\"blocks.{layer_index}.mlp.hook_post\"\n",
    "prev_layer_loc = f\"blocks.{layer_index}.ln2.hook_normalized\"\n",
    "\n",
    "ds = load_dataset(\"NeelNanda/pile-10k\", split='train[:10]')\n",
    "ds_tokens = model.to_tokens(ds['text'])\n",
    "ds_logits, ds_cache = model.run_with_cache(ds_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = tools.extract_mlp(model, layer_index)\n",
    "\n",
    "ds_acts = ds_cache[prev_layer_loc].numpy().reshape(-1, 768)\n",
    "original_mlp = tools.get_original_mlp_for_sparx(mlp, ds_acts)\n",
    "\n",
    "shrink_pcs = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "merged_models = []\n",
    "models_cluster_labels = []\n",
    "for pc in shrink_pcs:\n",
    "  model, labels = tools.shrink_model_global(original_mlp, pc)\n",
    "  model.model.summary()\n",
    "  merged_models.append(model)\n",
    "  models_cluster_labels.append(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example prompts from selected concepts with explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://www.neuronpedia.org/gpt2-small/6-mlp-oai/25423\n",
    "# phrases that denote a time span, with specific emphasis on the recent past\n",
    "\n",
    "over_past_concept_tokens = [\"over the past decade\", \"over the past year\", \"over the past few\", \"over the past nine years\", \"over the past century\"]\n",
    "for i, sparx_mlp in enumerate(merged_models):\n",
    "  print(f\"Cluster activations for model of sparsification {shrink_pcs[i]}\")\n",
    "  latent_features.cluster_activations_for_prompt(model, prev_layer_loc, over_past_concept_tokens, sparx_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from latent_features import cluster_activations_for_prompt, measure_similarity_in_activated_clusters\n",
    "sparsest_model = merged_models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.neuronpedia.org/gpt2-small/6-mlp-oai/24106\n",
    "# the word 'know'\n",
    "\n",
    "know_concept_tokens = [\"know\", \"I know this\", \"I know it\", \"I know nothing\", \"I know that\", \"I know there is a trend\", \"I know everyone\", \"I know I am\"]\n",
    "top_clusters_for_prompts = cluster_activations_for_prompt(model, prev_layer_loc, know_concept_tokens, sparsest_model)\n",
    "measure_similarity_in_activated_clusters(top_clusters_for_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.neuronpedia.org/gpt2-small/6-mlp-oai/2325\n",
    "# metaphorical phrases involving elements or materials, often related to energizing or de-energizing situations\n",
    "\n",
    "metaphors_concept_tokens = [\n",
    "    \"that decision provided fresh fuel for her\",\n",
    "    \"added fuel to the current battle\",\n",
    "    \"poured cold water on the plan\",\n",
    "    \"give me some food for thought\",\n",
    "    \"kernel of an idea germinating\",\n",
    "    \"taken with a grain of salt\"\n",
    "]\n",
    "top_clusters_for_prompts = cluster_activations_for_prompt(model, prev_layer_loc, metaphors_concept_tokens, sparsest_model)\n",
    "measure_similarity_in_activated_clusters(top_clusters_for_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.neuronpedia.org/gpt2-small/6-mlp-oai/5277\n",
    "# emotive or value-based terms related to personal commitment or giving\n",
    "\n",
    "give_concept_tokens = [\n",
    "    \"Want to give some love to your favorite cheap breakfast spot?\",\n",
    "    \"I've given my heart and soul to the network\",\n",
    "    \"I would like to thank and give the glory to God\",\n",
    "    \"give life to\",\n",
    "    \"give time and attention\",\n",
    "]\n",
    "top_clusters_for_prompts = cluster_activations_for_prompt(model, prev_layer_loc, give_concept_tokens, sparsest_model)\n",
    "measure_similarity_in_activated_clusters(top_clusters_for_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to raw neuron activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_activations_for_prompt(model, prev_layer_loc, know_concept_tokens, original_mlp)\n",
    "cluster_activations_for_prompt(model, prev_layer_loc, metaphors_concept_tokens, original_mlp)\n",
    "cluster_activations_for_prompt(model, prev_layer_loc, give_concept_tokens, original_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For randomly selected latent concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "NUM_LATENT_FEATURES = 32768\n",
    "\n",
    "# Focus on sparsest model for now\n",
    "sparsest_model = merged_models[0]\n",
    "\n",
    "feature_descriptions = {}\n",
    "feature_prompts = {}\n",
    "feature_prompt_clusters = {}\n",
    "\n",
    "while len(feature_descriptions) < 10: # todo: find optimal size\n",
    "  feature_index = random.randint(0, NUM_LATENT_FEATURES)\n",
    "  print(f\"latent: {feature_index}\")\n",
    "  desc, prompts = latent_features.get_top_activating_prompts_for_latent_feature(feature_index)\n",
    "  if len(desc) == 0 and len(prompts) == 0:\n",
    "    print(f\"Dead latent: {feature_index}\")\n",
    "    continue # Dead latent\n",
    "\n",
    "  feature_descriptions[feature_index] = desc\n",
    "  feature_prompts[feature_index] = prompts\n",
    "\n",
    "  print(desc, prompts)\n",
    "  prompt_clusters = latent_features.get_top_clusters_for_prompts(prompts, sparsest_model)\n",
    "  feature_prompt_clusters[feature_index] = prompt_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_index, prompts in feature_prompts.items():\n",
    "  print(feature_index)\n",
    "  cluster_activations_for_prompt(prompts, sparsest_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure similarity in cluster activations for prompts of the same latent feature\n",
    "\n",
    "for feature_index, prompt_clusters in feature_prompt_clusters.items():\n",
    "  sim = measure_similarity_in_activated_clusters(prompt_clusters)\n",
    "  print(f\"Feature {feature_index}: {feature_prompts[feature_index]}\")\n",
    "  print(f\"Clusters: {feature_prompt_clusters[feature_index]}\")\n",
    "  print(f\"Average similarity: {np.mean(sim)}\")\n",
    "  plt.imshow(sim)\n",
    "  plt.colorbar(label='similarity')\n",
    "  plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
